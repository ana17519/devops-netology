**Домашнее задание к занятию 17 «Инцидент-менеджмент»**

**Основная часть**

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информация о сбое:

* [в виде краткой выжимки на русском языке](https://habr.com/ru/articles/427301/);
* [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


**Постмортем:**

* краткое описание

В 22:52 (21.10) по UTC на нескольких сервисах GitHub.com пострадали несколько сетевых разделов и последующим сбоем базы данных, 
что привело к появлению непоследовательной информации на веб-сайте. Произошло снижение качества обслуживания на 24 часа 11 минут. 

* предшествующие события

21 октября в 22:52 UTC в результате работ по замене вышедшего из строя оптического оборудования 100G 
была потеряна связь между сетевым концентратором на восточном побережье США и основным центром 
обработки данных на восточном побережье США. Связь между этими точками была восстановлена за 43 секунды

* причина инцидента

Кратковременный сбой вызвал цепочку событий, которые привели к ухудшению качества обслуживания на 24 часа 11 минут.

* воздействие

Появление непоследовательной информации на веб-сайте, снижение качества обслуживания на 24 часа 11 минут

* обнаружение

2018 21 октября 22:54 UTC внутренние системы мониторинга начали генерировать предупреждения, указывающие на 
многочисленные сбои в системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений.
К 23:02 UTC инженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся 
в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только 
серверы из центра обработки данных на западном побережье США.

* реакция

2018 21 октября 23:07 UTC отвечающая команда решила вручную заблокировать внутренний инструмент развертывания, чтобы 
предотвратить внесение каких-либо дополнительных изменений. В 23:09 по всемирному координированному времени команда 
респондентов поместила сайт в желтый статус. Это действие автоматически переводит ситуацию в активный инцидент и отправляет 
предупреждение координатору инцидентов. В 23:11 по всемирному координированному времени присоединился координатор инцидента 
и через две минуты изменил статус решения на красный.
2018 21 октября 23:13 UTC было понятно, что проблема затронула несколько кластеров баз данных. инженеры начали исследовать 
текущее состояние, чтобы определить, какие действия 
необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера 
и перестроить топологию репликации. 

* восстановление

22 октября 2018 г., 06:51 UTC Несколько кластеров завершили восстановление из резервных копий в  центре обработки данных 
на восточном побережье США и начали репликацию новых данных с западного побережья. Это привело к медленной загрузке сайта для 
страниц, которые должны были выполнить операцию записи по межстрановой ссылке, но страницы, читающие из этих кластеров баз 
данных, возвращали актуальные результаты, если запрос на чтение попадал на только что восстановленную реплику. 
Другие более крупные кластеры баз данных все еще восстанавливались.

Команды определили способы восстановления непосредственно с Западного побережья, чтобы преодолеть ограничения пропускной способности, 
вызванные загрузкой из внешнего хранилища, и все больше убеждались в том, что восстановление неизбежно, а время, оставшееся 
до установления работоспособной топологии репликации, зависело от того, как долго она будет работать. 
Эта оценка была линейно интерполирована на основе имеющейся у нас телеметрии репликации, а страница состояния была обновлена, 
чтобы установить ожидаемое время восстановления в два часа.
22 октября 2018 г., 11:12 UTC Все первичные базы данных снова установлены на восточном побережье США. 
Это привело к тому, что сайт стал гораздо более отзывчивым, так как записи теперь направлялись на сервер базы данных, 
расположенный в том же физическом центре обработки данных, что и наш уровень приложений. Несмотря на то, что это существенно 
повысило производительность, по-прежнему существовали десятки реплик чтения базы данных, которые отставали от основной на несколько часов. 
Эти отложенные реплики приводили к тому, что пользователи видели несогласованные данные при взаимодействии с нашими службами. 
Нагрузка чтения распределена по большому пулу реплик чтения, и каждый запрос к службам имел хорошие шансы попасть в реплику чтения, 
которая задерживалась на несколько часов.


* таймлайн

Хронология инцидента

**2018 21 октября 22:52 UTC**

Orchestrator, который был активен в основном центре обработки данных, начал процесс отмены выбора руководства. 
Центр обработки данных на западном побережье США и узлы Orchestrator общедоступного облака на восточном побережье 
США смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр 
обработки данных на западном побережье США. Orchestrator приступил к организации топологии кластера базы данных 
Западного побережья США. Когда подключение было восстановлено, уровень приложений немедленно начал направлять т
рафик записи на новые первичные серверы на сайте West Coast.

Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который 
не был реплицирован на объект на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки 
данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить 
возврат основного сервера в центр обработки данных на восточном побережье США.

**2018 21 октября 22:54 UTC**

 внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах. 
 В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. 
 К 23:02 UTC инженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся 
 в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только 
 серверы из центра обработки данных на западном побережье США.

**2018 21 октября 23:07 UTC**

К этому моменту отвечающая команда решила вручную заблокировать наш внутренний инструмент развертывания, чтобы 
предотвратить внесение каких-либо дополнительных изменений. В 23:09 по всемирному координированному времени команда 
респондентов поместила сайт в желтый статус . Это действие автоматически переводит ситуацию в активный инцидент и 
отправляет предупреждение координатору инцидентов. В 23:11 по всемирному координированному времени присоединился 
координатор инцидента и через две минуты изменил статус решения на красный .

**2018 21 октября 23:13 UTC**

В то время было понятно, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры 
из группы разработки баз данных GitHub. Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо 
предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и 
перестроить топологию репликации. Это было непросто, потому что к этому моменту кластер базы данных West Coast уже 
почти 40 минут принимал записи с нашего уровня приложений. Кроме того, в кластере Восточного побережья существовало 
несколько секунд операций записи, которые не были реплицированы на Западное побережье, что предотвратило репликацию 
новых операций записи обратно на Восточное побережье.

**2018 21 октября 23:19 UTC**

Из запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих 
метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично ухудшить удобство использования сайта, 
приостановив доставку веб-перехватчика и сборки GitHub Pages вместо того, чтобы подвергать опасности данные, которые мы 
уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы отдавать предпочтение целостности 
данных, а не удобству использования сайта и времени восстановления.

**22 октября 2018 г., 00:05 UTC**
Инженеры, участвующие в группе реагирования на инциденты, начали разработку плана по устранению несоответствий данных и 
внедрению наших процедур аварийного переключения для MySQL. Наш план состоял в том, чтобы восстановить из резервных копий, 
синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий
в очереди. Мы обновили наш статус , чтобы проинформировать пользователей о том, что мы собираемся выполнить контролируемый 
переход на другой ресурс внутренней системы хранения данных.

**22 октября 2018 г., 00:41 UTC**

К этому времени был инициирован процесс резервного копирования для всех затронутых кластеров MySQL, и инженеры следили 
за его ходом. Одновременно несколько групп инженеров искали способы ускорить передачу и время восстановления без дальнейшего 
ухудшения удобства использования сайта или риска повреждения данных.

**22 октября 2018 г., 06:51 UTC**

Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье 
США и начали репликацию новых данных с западного побережья. Это привело к медленной загрузке сайта для страниц, которые 
должны были выполнить операцию записи по межстрановой ссылке, но страницы, читающие из этих кластеров баз данных, возвращали 
актуальные результаты, если запрос на чтение попадал на только что восстановленную реплику. Другие более крупные кластеры 
баз данных все еще восстанавливались.

**22 октября 2018 07:46 UTC**

GitHub опубликовал сообщение в блоге , чтобы предоставить больше контекста. Мы используем GitHub Pages для внутренних целей, 
и все сборки были приостановлены несколькими часами ранее, поэтому публикация этого потребовала дополнительных усилий. 
Мы приносим извинения за задержку. Мы намеревались разослать это сообщение намного раньше и позаботимся о том, чтобы 
в будущем мы могли публиковать обновления с учетом этих ограничений.

**22 октября 2018 г., 11:12 UTC**

Все первичные базы данных снова установлены на восточном побережье США. Это привело к тому, что сайт стал гораздо
более отзывчивым, так как записи теперь направлялись на сервер базы данных, расположенный в том же физическом центре
обработки данных, что и наш уровень приложений. Несмотря на то, что это существенно повысило производительность, 
по-прежнему существовали десятки реплик чтения базы данных, которые отставали от основной на несколько часов. 
Эти отложенные реплики приводили к тому, что пользователи видели несогласованные данные при взаимодействии с 
нашими службами. Мы распределили нагрузку чтения по большому пулу реплик чтения, и каждый запрос к нашим службам 
имел хорошие шансы попасть в реплику чтения, которая задерживалась на несколько часов.

**22 октября 2018 г., 16:24 UTC**
Как только реплики были синхронизированы, мы выполнили аварийное переключение на исходную топологию, решив немедленные 
проблемы с задержкой/доступностью. В рамках сознательного решения отдавать предпочтение целостности данных, а не более 
короткому периоду инцидента, мы сохранили статус службы красным , пока начали обрабатывать накопившиеся данные.

**22 октября 2018 г., 16:45 (всемирное координированное время)**
На этом этапе восстановления нам нужно было сбалансировать возросшую нагрузку, представленную отставанием, 
потенциально перегружая наших партнеров по экосистеме уведомлениями, и как можно быстрее вернуть наши услуги 
на 100%. В очереди было более пяти миллионов событий ловушек и 80 тысяч сборок страниц.

**22 октября 2018 23:03 UTC**

Все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа 
всех систем. Статус сайта был обновлен до зеленого.

* последующие действия

**Устранение несоответствий данных**

Во время восстановления захватили двоичные журналы MySQL, содержащие записи, которые были сделаны на 
 основном сайте, но которые не были реплицированы на  сайт Западного побережья из каждого затронутого кластера. 
Общее количество операций записи, которые не были воспроизведены на Западном побережье, было относительно небольшим. 
Например, один из  самых загруженных кластеров имел 954 записи в затронутом окне. Проводится
анализ этих журналов и определение, какие записи могут быть автоматически согласованы, а какие потребуют взаимодействия 
с пользователями. Есть несколько команд, занятых этой работой, и  анализ уже определил категорию записей, 
которые с тех пор повторялись пользователем и успешно сохранялись. Как указано в этом анализе,  основная цель — 
сохранить целостность и точность данных, которые вы храните на GitHub.

**Коммуникация**

Было сделано несколько публичных оценок времени ремонта на 
основе скорости обработки невыполненных данных, однако оценки не учитывали все переменные. 

**Технические инициативы**

В ходе этого анализа был выявлен ряд технических инициатив.

1. Настроить конфигурацию Orchestrator, чтобы предотвратить продвижение основных баз данных через региональные границы. 
Действия Оркестратора вели себя так, как настроено, несмотря на то, что уровень приложений не смог поддержать 
это изменение топологии. Выборы лидеров в регионе, как правило, безопасны, но внезапная задержка между странами стала основным
фактором, способствовавшим этому инциденту. Это было неожиданное поведение системы, учитывая, что ранее не видели внутреннего 
сетевого раздела такого масштаба.
2. Ускорен переход на новый механизм отчетов о состоянии, который предоставит более богатый форум для обсуждения 
активных инцидентов более четким и понятным языком.
3. За несколько недель до этого инцидента началась общекорпоративная инженерная инициатива по поддержке обслуживания 
трафика GitHub из нескольких центров обработки данных в схеме «активный/активный/активный».
Цель этой работы — допустить полный отказ одного центра обработки данных 
без воздействия на пользователя. Этот инцидент добавил актуальности инициативе.

**Организационные инициативы**

Этот инцидент изменил отношение к надежности сайта. Узнали, что более строгий операционный контроль или сокращение 
времени отклика не являются достаточными гарантиями надежности сайта в рамках системы служб. 

